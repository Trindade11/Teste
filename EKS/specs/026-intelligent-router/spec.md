# Spec 026: Intelligent Router System

**Feature**: Sistema de Roteamento Inteligente  
**Priority**: P1 (High)  
**Sprint**: 3 ou 4  
**Effort**: 4 dias  
**Status**: ðŸ“‹ Planned  

---

## VisÃ£o Geral

Sistema que otimiza **custo, latÃªncia e qualidade** atravÃ©s de:
1. **Context Depth Control** - Profundidade ajustÃ¡vel de contexto
2. **LLM Router** - SeleÃ§Ã£o automÃ¡tica do modelo ideal
3. **User Learning** - Aprende preferÃªncias ao longo do tempo

---

## Problema

Atualmente, sistemas usam sempre o mesmo modelo (ex: GPT-4o) para todas as queries, resultando em:
- âŒ **Alto custo** - Usa modelo caro mesmo para queries simples
- âŒ **LatÃªncia desnecessÃ¡ria** - Busca contexto profundo sempre
- âŒ **ExperiÃªncia genÃ©rica** - NÃ£o se adapta ao usuÃ¡rio

---

## SoluÃ§Ã£o

### 1. Context Depth Control

**3 nÃ­veis de profundidade**:

```yaml
Level 1 - Surface (RÃ¡pido):
  latency: ~500ms
  tokens: ~500
  scope: Resposta direta
  use_case: "Qual o status da startup X?"

Level 2 - Contextual (Balanceado):
  latency: ~2s
  tokens: ~2000
  scope: + HistÃ³rico recente + Relacionamentos
  use_case: "Por que rejeitamos a startup X?"

Level 3 - Deep Corporate (Profundo):
  latency: ~5s
  tokens: ~8000
  scope: + HistÃ³rico completo + DecisÃµes similares + EstratÃ©gia
  use_case: "Como nossa tese de HealthTech evoluiu?"
```

**Aprendizado automÃ¡tico**:
- Sistema aprende qual profundidade o usuÃ¡rio prefere por tipo de query
- Armazena no Neo4j: `(:User)-[:PREFERS_DEPTH {query_type, depth, confidence}]`

### 2. LLM Router

**SeleÃ§Ã£o inteligente de modelo**:

```typescript
Query Classification â†’ Model Selection:

Simples/Factual        â†’ GPT-4o-mini ($0.15/1M)
Reasoning Complexo     â†’ o1-preview  ($15/1M)
CÃ³digo                 â†’ Claude Sonnet ($3/1M)
EstratÃ©gico/Balanceado â†’ GPT-4o      ($5/1M)
```

**Economia esperada**: 40-60% vs sempre usar GPT-4o

### 3. API Gateway

Endpoints consolidados (ver `_context/API-DESIGN.md`)

---

## Fluxo de Processo

```mermaid
sequenceDiagram
    participant User
    participant API as Backend API
    participant Classifier as Query Classifier
    participant DepthEngine as Depth Engine
    participant LLMRouter as LLM Router
    participant Neo4j
    participant LLM as Selected LLM
    
    User->>API: POST /chat/query
    API->>Classifier: Classify query
    Classifier-->>API: {type, complexity, estimated_tokens}
    
    API->>DepthEngine: Get preferred depth
    DepthEngine->>Neo4j: Load user preferences
    Neo4j-->>DepthEngine: {preferred_depth: 2}
    
    API->>Neo4j: Retrieve context (depth=2)
    Neo4j-->>API: Context chunks
    
    API->>LLMRouter: Select model
    LLMRouter-->>API: {model: "gpt-4o-mini", cost, latency}
    
    API->>LLM: Execute query
    LLM-->>API: Response
    
    API->>Neo4j: Record interaction (learning)
    API-->>User: Response + metadata
```

---

## Requisitos Funcionais

### RF-01: Context Depth Manual
- UsuÃ¡rio pode especificar `depth_level: 1|2|3` no request
- Sistema retorna metadata de quantos tokens/chunks foram usados

### RF-02: Context Depth Auto
- Se `depth_level: "auto"`, sistema usa preferÃªncia aprendida
- Aprende atravÃ©s de feedback implÃ­cito (tokens lidos, tempo de leitura)

### RF-03: LLM Classification
- Classificador rÃ¡pido (GPT-4o-mini) analisa query
- Retorna: `{type, complexity, estimated_tokens, requires_tools}`

### RF-04: Model Selection
- Baseado em classificaÃ§Ã£o + preferÃªncias do usuÃ¡rio
- Considera: custo, velocidade, qualidade
- Permite override manual: `model_preference: "gpt-4o"`

### RF-05: Cost Tracking
- Registra custo de cada query no MongoDB
- Dashboard admin mostra: cost/day, cost/user, savings

### RF-06: Learning Loop
- Sistema aprende depth preferences
- Atualiza confidence score baseado em interaÃ§Ãµes
- Neo4j: `(:User)-[:PREFERS_DEPTH {confidence, learned_from}]`

---

## User Scenarios

### CenÃ¡rio 1: UsuÃ¡rio Executivo (Prefere Rapidez)

```
Query: "Status da Startup ABC?"

Sistema aprende:
- UsuÃ¡rio quase sempre aceita depth=1
- Tempo mÃ©dio de leitura: 5 segundos
- Raramente pede mais contexto

Behavior:
depth=1 automaticamente â†’ GPT-4o-mini â†’ Resposta em 500ms
```

### CenÃ¡rio 2: Analista (Prefere Contexto)

```
Query: "Por que rejeitamos Startup XYZ?"

Sistema aprende:
- UsuÃ¡rio quase sempre pede depth=3
- LÃª contexto completo (>30s)
- Faz perguntas de follow-up

Behavior:
depth=3 automaticamente â†’ GPT-4o â†’ Resposta em 5s com contexto completo
```

### CenÃ¡rio 3: Query Complexa (Reasoning)

```
Query: "Compare nossa estratÃ©gia de HealthTech com concorrentes e sugira melhorias"

Classifier detecta:
- type: "strategic"
- complexity: 5
- requires_tools: true

LLM Router:
- Seleciona o1-preview (deep reasoning)
- depth=3 (contexto completo)
- Multi-agent team
```

---

## Entidades Principais

### Neo4j

```cypher
// User Depth Preferences
(:User)-[:PREFERS_DEPTH {
  query_type: "factual" | "decision" | "strategic",
  preferred_depth: 1 | 2 | 3,
  confidence: float,
  learned_from_interactions: int,
  last_updated: datetime
}]->(:ContextStrategy)

// Query History (for learning)
(:QueryLog {
  id: uuid,
  user_id: string,
  query: string,
  classification: object,
  depth_used: int,
  model_used: string,
  tokens_used: int,
  latency_ms: int,
  cost_usd: float,
  user_feedback: "too_shallow" | "too_deep" | "just_right",
  timestamp: datetime
})
```

### MongoDB (Cost Tracking)

```typescript
interface CostLog {
  _id: ObjectId;
  user_id: string;
  query_id: string;
  model: string;
  tokens_input: number;
  tokens_output: number;
  cost_usd: number;
  timestamp: Date;
}

// TTL index: 90 days
db.cost_logs.createIndex({ timestamp: 1 }, { expireAfterSeconds: 7776000 });
```

---

## APIs

Ver detalhamento completo em: `_context/API-DESIGN.md`

**Principais endpoints**:
- `POST /chat/query` - Com depth + model routing
- `POST /llm/route` - SeleÃ§Ã£o de modelo
- `GET /user/:userId/preferences` - PreferÃªncias aprendidas
- `POST /user/:userId/learn-depth` - Feedback para aprendizado

---

## MÃ©tricas de Sucesso

### EficiÃªncia
- âœ… ReduÃ§Ã£o de custo: **40-60%** vs sempre GPT-4o
- âœ… LatÃªncia mÃ©dia: **<2s** para depth=1/2
- âœ… Accuracy classificador: **>90%**

### User Experience
- âœ… Depth learning accuracy: **>80%** apÃ³s 20 interaÃ§Ãµes
- âœ… User satisfaction: **>85%** (feedback)

### Cost Savings
```
Baseline (sempre GPT-4o): $5/1M tokens input
Otimizado:
  - 40% queries â†’ GPT-4o-mini: $0.15/1M
  - 40% queries â†’ GPT-4o: $5/1M
  - 20% queries â†’ o1-preview: $15/1M

Custo mÃ©dio: $4.21/1M tokens
Economia: 16% + melhor qualidade em queries complexas
```

---

## RestriÃ§Ãµes TÃ©cnicas

1. **Classificador deve ser rÃ¡pido**: <200ms
2. **Aprendizado deve convergir**: <20 interaÃ§Ãµes
3. **Fallback**: Se classifier falha, usa GPT-4o + depth=2
4. **Override sempre disponÃ­vel**: UsuÃ¡rio pode forÃ§ar modelo/depth

---

## Dependencies

| Spec | Dependency | Reason |
|------|------------|--------|
| 005 | **MUST** | Agent Router base |
| 017 | **MUST** | Memory ecosystem |
| 024 | **SHOULD** | Retrieval orchestration |
| 015 | **MUST** | Neo4j graph model |

---

## Implementation Notes

### Phase 1: Classification (2d)
```typescript
// src/services/QueryClassifier.ts
class QueryClassifier {
  async classify(query: string): Promise<QueryClassification> {
    // Use GPT-4o-mini for fast classification
  }
}
```

### Phase 2: Depth Engine (1d)
```typescript
// src/services/DepthEngine.ts
class DepthEngine {
  async getPreferredDepth(userId: string, queryType: string): Promise<number> {
    // Load from Neo4j, use confidence scores
  }
  
  async learn(userId: string, feedback: DepthFeedback): Promise<void> {
    // Update preferences
  }
}
```

### Phase 3: LLM Router (1d)
```typescript
// src/services/LLMRouter.ts
class LLMRouter {
  selectModel(classification: QueryClassification): string {
    // Logic from API-DESIGN.md
  }
}
```

---

## Testing Strategy

```typescript
describe('Intelligent Router', () => {
  it('classifies simple query correctly', async () => {
    const result = await classifier.classify("Status da Startup X?");
    expect(result.type).toBe("factual");
    expect(result.complexity).toBeLessThanOrEqual(2);
  });
  
  it('learns depth preference', async () => {
    // Simulate 10 interactions with depth=1
    for (let i = 0; i < 10; i++) {
      await depthEngine.recordInteraction(userId, {depth: 1, feedback: "just_right"});
    }
    
    const pref = await depthEngine.getPreferredDepth(userId, "factual");
    expect(pref).toBe(1);
  });
  
  it('selects cheaper model for simple queries', async () => {
    const classification = {type: "factual", complexity: 1};
    const model = llmRouter.selectModel(classification);
    expect(model).toBe("gpt-4o-mini");
  });
});
```

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Classifier incorreto | Alto | Fallback para GPT-4o, usuÃ¡rio pode override |
| Depth errado | MÃ©dio | Feedback loop rÃ¡pido, ajusta em <5 interaÃ§Ãµes |
| Custo nÃ£o reduz | MÃ©dio | Monitorar dashboard, ajustar thresholds |

---

**Status**: ðŸ“‹ Planned  
**Next Steps**: Adicionar ao Sprint 3 apÃ³s Agent Router (005) funcional
