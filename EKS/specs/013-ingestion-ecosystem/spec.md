# Feature Specification: Ingestion Ecosystem (Documents & Data)

**Feature Branch**: `013-ingestion-ecosystem`  
**Created**: 2025-12-07  
**Status**: Draft  
**Priority**: P1 (Core)  
**Source**: TRG-SPC-20251206-003 (Document Pipeline) + TRG-SPC-20251206-011 (Ingestion Ecosystem) + chat.txt + specs 001/007/010/012

## Process Flow (Business View)

```mermaid
flowchart TD
    subgraph Sources["üì• Fontes de Ingest√£o"]
        ChatUpload["üí¨ Upload via Chat"]
        AdminUpload["üßë‚Äçüíº Upload Admin
(painel)"]
        APIIn["üîå APIs Externas"]
        Forms["üìã Formul√°rios
(Onboarding, Config)"]
        Meetings["üéôÔ∏è Grava√ß√µes de Reuni√£o
(√°udio/v√≠deo)"]
    end

    subgraph Intake["üö™ Camada de Entrada"]
        IntakeAPI["/ingest API"]
        Queue["üì® Fila de Processamento"]
        FileStore["üóÑÔ∏è Armazenamento de Arquivos Brutos"]
    end

    subgraph Processing["‚öôÔ∏è Processamento de Documentos"]
        Docling["üìë Docling
(extra√ß√£o & chunking)"]
        DetectType["üîç Detectar Tipo de Documento"]
        Enrich["‚ú® Enriquecer Metadados
(autor, tags, projeto)"]
    end

    subgraph Pipeline["ü§ñ Pipeline de Conhecimento (001)"]
        Filtration["üîç Filtra√ß√£o Real vs Passageiro (010)"]
        MemoryDecision["üß† Decis√£o de Mem√≥ria
Corp vs Pessoal (009)"]
        Curation["üßπ Curadoria do Grafo (012)"]
    end

    subgraph Storage["üíæ Armazenamento"]
        RawBucket["üóÉÔ∏è Bucket Bruto
(arquivos)"]
        StagingGraph["üß™ Staging Graph"]
        MainGraph["üóÇÔ∏è Neo4j Principal"]
    end

    Sources --> IntakeAPI
    IntakeAPI --> Queue
    Queue --> FileStore
    FileStore --> Docling

    Docling --> DetectType
    DetectType --> Enrich
    Enrich --> Filtration

    Filtration --> MemoryDecision
    MemoryDecision --> Curation
    Curation --> StagingGraph
    StagingGraph --> MainGraph

    FileStore --> RawBucket

    classDef src fill:#e3f2fd,stroke:#1976d2,color:#000
    classDef intake fill:#fff3e0,stroke:#ff9800,color:#000
    classDef proc fill:#e8f5e9,stroke:#4caf50,color:#000
    classDef pipe fill:#fce4ec,stroke:#e91e63,color:#000
    classDef store fill:#f3e5f5,stroke:#9c27b0,color:#000

    class ChatUpload,AdminUpload,APIIn,Forms,Meetings src
    class IntakeAPI,Queue,FileStore intake
    class Docling,DetectType,Enrich proc
    class Filtration,MemoryDecision,Curation pipe
    class RawBucket,StagingGraph,MainGraph store
```

### Papel do Ecossistema de Ingest√£o

- Ponto **√∫nico de entrada** de dados estruturados/n√£o estruturados.  
- Garante que **todo dado** passe por: 
  1) Certifica√ß√£o Docling/Parsing ‚Üí  
  2) Filtra√ß√£o Real/Passageiro (010) ‚Üí  
  3) Decis√£o de Mem√≥ria (009) ‚Üí  
  4) Curadoria do Grafo (012).  
- D√° visibilidade e controle: *o que entrou, de onde veio, como foi processado*.

---

## Agent Collaboration

```mermaid
flowchart TD
    User["üë§ User/Admin/API"] --> IntakeAPI["üö™ Ingestion API"]

    IntakeAPI --> IngestionAgent["ü§ñ Ingestion Orchestrator"]

    subgraph DocPipeline["üìë Document Pipeline (Docling)"]
        IngestionAgent --> DoclingAgent["Docling Agent"]
        DoclingAgent --> Chunker["Chunking & Table‚ÜíJSON"]
    end

    Chunker --> KnowledgePipeline["ü§ñ Knowledge Pipeline (001)"]
    KnowledgePipeline --> FiltrationAgent["üîç Data Filtration (010)"]
    FiltrationAgent --> MemoryDecisionAgent["üß† Memory Decision (009)"]
    MemoryDecisionAgent --> CurationAgents["üßπ Curation Ecosystem (012)"]
```

- **Ingestion Orchestrator**: coordena o fluxo t√©cnico (fila, Docling, chamadas para pipeline).  
- **Docling Agent**: encapsula intera√ß√£o com Docling (local ou servi√ßo externo).  
- **Knowledge/Filtration/Decision/Curation Agents**: j√° especificados em specs anteriores.

---

## User Scenarios & Testing

### User Story 1 - Upload de Documento via Chat (Priority: P1)

Usu√°rio anexa PDF de reuni√£o no chat; sistema processa, chama Docling, e inicia pipeline completo.

**Acceptance Scenarios**:

1. **Given** usu√°rio no chat, **When** anexa PDF, **Then** arquivo √© enviado para Ingestion API e guardado em FileStore/RawBucket com metadados (user_id, company_id, filename, size).

2. **Given** arquivo salvo, **When** Ingestion Orchestrator l√™ fila, **Then** chama Docling para extra√ß√£o, obtendo texto, chunks, tabelas (como JSON) e metadados.

3. **Given** chunks gerados, **When** pipeline 001 roda, **Then** dados passam por Filtra√ß√£o (010), Decis√£o de Mem√≥ria (009) e Curadoria (012), resultando em nodes `:Document`, `:Chunk`, `:Knowledge`, `:Task` conforme aplic√°vel.

---

### User Story 2 - Ingest√£o em Lote por Admin (Priority: P1)

Admin sobe um ZIP com 50 documentos; ingest√£o acontece em lote com monitoramento de status.

**Acceptance Scenarios**:

1. **Given** admin no painel, **When** faz upload de ZIP, **Then** Ingestion API cria `:IngestionBatch` e m√∫ltiplos `:IngestionItem` (um por arquivo).

2. **Given** batch criado, **When** processamento inicia, **Then** admin consegue ver progresso (processando, conclu√≠do, erro) por item.

3. **Given** alguns documentos falham, **When** admin abre detalhes, **Then** v√™ motivo (erro Docling, tipo n√£o suportado, etc.) logado.

---

### User Story 3 - Ingest√£o via API Externa (Priority: P2)

Sistema cliente envia dados via API (JSON) para ingest√£o; pipeline trata como fonte estruturada.

**Acceptance Scenarios**:

1. **Given** integra√ß√£o externa registrada, **When** POST `/ingest/api` √© chamado com payload JSON, **Then** Ingestion API valida autentica√ß√£o, formato e cria `:IngestionItem` do tipo `api`.

2. **Given** item API v√°lido, **When** pipeline roda, **Then** dados s√£o normalizados, classificados Real/Passageiro e potencialmente viram knowledge, tasks, etc.

---

## Functional Requirements

### Entry Points

**REQ-ING-001**: Sistema DEVE expor Ingestion API unificada para: uploads de chat, uploads admin, formul√°rios e APIs externas.  
**REQ-ING-002**: Cada requisi√ß√£o de ingest√£o DEVE gerar pelo menos um `:IngestionItem` com metadados b√°sicos (source_type, source_ref, user_id, company_id).

### Document Pipeline (Docling)

**REQ-ING-003**: Para documentos, sistema DEVE usar Docling para:
- Extra√ß√£o de texto.  
- Chunking em `:Chunk` com preserva√ß√£o de ordem.  
- Convers√£o de tabelas em JSON.  

**REQ-ING-004**: Documentos DEVEM ser taggeados com tipo (`meeting`, `report`, `email`, `note`, etc.), seja por heur√≠stica ou LLM.

### Integration with Knowledge Pipeline

**REQ-ING-005**: Ap√≥s Docling, chunks/documents DEVEM ser enviados ao Knowledge Pipeline (001).  
**REQ-ING-006**: Knowledge Pipeline DEVE chamar Filtra√ß√£o (010) e Decis√£o de Mem√≥ria (009) antes da Curadoria (012).

### Monitoring & Status

**REQ-ING-007**: Sistema DEVE permitir rastrear status por `:IngestionBatch` e `:IngestionItem` (pending, processing, completed, failed).  
**REQ-ING-008**: Erros de ingest√£o DEVEM ser registrados com causa (erro Docling, arquivo corrompido, timeout, etc.).

### Performance & Limits

**REQ-ING-009**: Ingest√£o deve suportar pelo menos N documentos simult√¢neos (N a definir no plano, ex: 100) sem travar.  
**REQ-ING-010**: Tamanho m√°ximo de arquivo (ex: 50MB) e tipos suportados DEVEM ser documentados e validados na entrada.

---

## Success Criteria

- ‚úÖ 95%+ dos documentos v√°lidos submetidos s√£o processados com sucesso.  
- ‚úÖ Tempo m√©dio de ingest√£o para documento padr√£o (<10MB) < 10s (extra√ß√£o + pipeline).  
- ‚úÖ Admin consegue ver claramente o que foi ingerido, seu status e destino no grafo.

---

## Key Entities (Neo4j)

```cypher
// IngestionBatch (upload em lote)
(:IngestionBatch {
  id: string,
  createdBy: string, // user.id
  source: string, // "admin_upload" | "api" | "chat"
  totalItems: integer,
  processedItems: integer,
  failedItems: integer,
  status: string, // "pending" | "processing" | "completed" | "failed"
  createdAt: datetime,
  updatedAt: datetime
})

// IngestionItem (unidade m√≠nima)
(:IngestionItem {
  id: string,
  batchId: string,
  sourceType: string, // "document" | "api" | "form" | "chat_upload"
  sourceRef: string, // file path, api record id, etc.
  userId: string,
  companyId: string,
  status: string, // "pending" | "processing" | "completed" | "failed"
  errorMessage: string,
  createdAt: datetime,
  updatedAt: datetime
})

(:IngestionBatch)-[:HAS_ITEM]->(:IngestionItem)
```

---

## Technical Constraints

- **Zero Hardcode**: tipos de documentos, limites de tamanho e regras de roteamento devem ser configur√°veis (n√£o fixos em c√≥digo).  
- Ingestion Orchestrator deve ser desacoplado (pode ser worker/servi√ßo separado, n√£o travar UI).  
- Logs de ingest√£o podem ser usados para auditoria, mas devem ter reten√ß√£o configur√°vel.

---

## Assumptions

1. Docling √© o motor principal de processamento de documentos; se falhar, n√£o h√° fallback no MVP (apenas erro claro e logado).  
2. Armazenamento bruto (RawBucket) pode ser em servi√ßo externo (S3/Azure Blob), abstra√≠do no backend.  
3. Integra√ß√µes futuras (ex: email, Slack) usar√£o mesma Ingestion API.

---

## Related Specs

- **001-knowledge-pipeline**: Define como dados processados viram conhecimento.  
- **007-chat-knowledge-capture**: Uploads via chat entram por este ecossistema.  
- **010-data-filtration**: Camada obrigat√≥ria ap√≥s processamento.  
- **012-graph-curation-ecosystem**: Recebe dados j√° filtrados e decide como entram no grafo.

---

## References

- `database-schema.md`: Nodes `Document`, `Chunk`, `Knowledge`.  
- `Spec-Orchestrator/.specify/triage/triage_specification.md`: Entries TRG-SPC-20251206-003 e 011.  
- Projeto refer√™ncia "CVCHub - Copia" para padr√µes de ingest√£o/Neo4j.
